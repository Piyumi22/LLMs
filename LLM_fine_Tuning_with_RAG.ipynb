{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFKKSqvoOokeGErCXD2Myw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Piyumi22/LLMs/blob/main/LLM_fine_Tuning_with_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uq2r6Ro3d8b7",
        "outputId": "6176f619-af46-4d99-e63b-6df5ef7b2238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mGPU available: False\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers datasets peft sentence-transformers scikit-learn torch pandas\n",
        "\n",
        "# Enable GPU acceleration\n",
        "# Go to Runtime -> Change runtime type -> Select \"T4 GPU\"\n",
        "import torch\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# 1. Setup & Imports\n",
        "# ======================\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModel,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import evaluate\n",
        "import torch\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# ======================\n",
        "# 2. Data Preparation\n",
        "# ======================\n",
        "# Load dataset (Colab-friendly version)\n",
        "dataset = load_dataset('imdb', split='train[:1000]+test[:1000]')\n",
        "dataset = DatasetDict({\n",
        "    'train': dataset.select(range(1000)),\n",
        "    'validation': dataset.select(range(1000, 2000))\n",
        "})\n",
        "\n",
        "# Create knowledge base\n",
        "knowledge_base = [\n",
        "    \"Positive reviews often contain words like excellent, amazing, wonderful.\",\n",
        "    \"Negative reviews often contain words like terrible, awful, disappointing.\",\n",
        "    \"Movies with great acting tend to get positive reviews.\",\n",
        "    \"Poor cinematography often leads to negative reviews.\",\n",
        "    \"Positive reviews frequently mention being entertained or moved.\",\n",
        "    \"Negative reviews often complain about plot holes or bad pacing.\"\n",
        "]\n",
        "\n",
        "# ======================\n",
        "# 3. RAG Setup\n",
        "# ======================\n",
        "# Initialize embedding model (faster than full BERT)\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Embed knowledge base\n",
        "knowledge_embeddings = embedding_model.encode(knowledge_base)\n",
        "\n",
        "# Create nearest neighbors index\n",
        "neigh = NearestNeighbors(n_neighbors=2)\n",
        "neigh.fit(knowledge_embeddings)\n",
        "\n",
        "def retrieve_relevant_info(text):\n",
        "    \"\"\"Retrieve relevant context for input text\"\"\"\n",
        "    query_embedding = embedding_model.encode([text])\n",
        "    _, indices = neigh.kneighbors(query_embedding)\n",
        "    return \" \".join([knowledge_base[i] for i in indices[0]])\n",
        "\n",
        "# ======================\n",
        "# 4. Model Setup\n",
        "# ======================\n",
        "model_checkpoint = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Define label maps\n",
        "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
        "label2id = {\"Negative\": 0, \"Positive\": 1}\n",
        "\n",
        "# Load models\n",
        "base_model = AutoModel.from_pretrained(model_checkpoint)\n",
        "classifier_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# 5. Custom RAG Model\n",
        "# ======================\n",
        "class RAGClassifier(torch.nn.Module):\n",
        "    def __init__(self, base_model, classifier_model):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.classifier_model = classifier_model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, retrieved_context=None):\n",
        "        # Process original text\n",
        "        text_output = self.base_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        ).last_hidden_state[:, 0, :]  # CLS token\n",
        "\n",
        "        # Process retrieved context if provided\n",
        "        if retrieved_context:\n",
        "            context_inputs = tokenizer(\n",
        "                retrieved_context,\n",
        "                return_tensors='pt',\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            ).to(input_ids.device)\n",
        "\n",
        "            context_output = self.base_model(\n",
        "                input_ids=context_inputs['input_ids'],\n",
        "                attention_mask=context_inputs['attention_mask']\n",
        "            ).last_hidden_state[:, 0, :]\n",
        "\n",
        "            # Combine features\n",
        "            combined = torch.cat([text_output, context_output], dim=1)\n",
        "        else:\n",
        "            combined = text_output\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier_model.classifier(combined)\n",
        "        return logits\n",
        "\n",
        "# Initialize model\n",
        "rag_model = RAGClassifier(base_model, classifier_model).to('cuda')\n",
        "\n",
        "# ======================\n",
        "# 6. Data Processing\n",
        "# ======================\n",
        "def tokenize_with_rag(examples):\n",
        "    # Retrieve context (batched for efficiency)\n",
        "    contexts = [retrieve_relevant_info(text) for text in examples[\"text\"]]\n",
        "\n",
        "    # Tokenize main text\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,  # Reduced for Colab memory\n",
        "        return_tensors=\"np\"\n",
        "    )\n",
        "\n",
        "    # Add labels and context\n",
        "    tokenized[\"labels\"] = examples[\"label\"]\n",
        "    tokenized[\"retrieved_context\"] = contexts\n",
        "    return tokenized\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_with_rag, batched=True)\n",
        "\n",
        "# ======================\n",
        "# 7. Training Setup\n",
        "# ======================\n",
        "peft_config = LoraConfig(\n",
        "    task_type=\"SEQ_CLS\",\n",
        "    r=4,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.01,\n",
        "    target_modules=['q_lin']\n",
        ")\n",
        "\n",
        "model = get_peft_model(rag_model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Evaluation metric\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}\n",
        "\n",
        "# Training arguments optimized for Colab\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"rag-lora-imdb\",\n",
        "    learning_rate=1e-3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,  # Reduced for Colab\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Custom data collator\n",
        "class RAGDataCollator:\n",
        "    def __call__(self, features):\n",
        "        batch = {\n",
        "            \"input_ids\": torch.stack([torch.tensor(f[\"input_ids\"]) for f in features]),\n",
        "            \"attention_mask\": torch.stack([torch.tensor(f[\"attention_mask\"]) for f in features]),\n",
        "            \"labels\": torch.tensor([f[\"labels\"] for f in features]),\n",
        "            \"retrieved_context\": [f[\"retrieved_context\"] for f in features]\n",
        "        }\n",
        "        return batch\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=RAGDataCollator()\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# 8. Train & Evaluate\n",
        "# ======================\n",
        "trainer.train()\n",
        "\n",
        "# ======================\n",
        "# 9. Inference Demo\n",
        "# ======================\n",
        "def predict(text):\n",
        "    context = retrieve_relevant_info(text)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            retrieved_context=[context]\n",
        "        )\n",
        "        pred = torch.argmax(logits).item()\n",
        "\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Retrieved Context: {context}\")\n",
        "    print(f\"Prediction: {id2label[pred]}\\n\")\n",
        "\n",
        "# Test examples\n",
        "test_texts = [\n",
        "    \"The acting was phenomenal and the story moved me to tears.\",\n",
        "    \"Worst movie I've ever seen, complete waste of time.\",\n",
        "    \"The cinematography was beautiful but the plot made no sense.\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    predict(text)"
      ],
      "metadata": {
        "id": "NdqsoM4NeBj-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}